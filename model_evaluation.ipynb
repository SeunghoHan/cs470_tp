{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "\n",
    "from data.load_datasets import *\n",
    "from experiment.utils import *\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from nlgeval import NLGEval\n",
    "\n",
    "import models.model_parameters as params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlgeval = NLGEval()  # loads the evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "torch.nn.Module.dump_patches = True\n",
    "\n",
    "checkpoint = params.checkpoint\n",
    "checkpoint = torch.load(os.path.join(params.ckpt_folder, checkpoint), map_location = params.device)\n",
    "vs_att_decoder = checkpoint['decoder']\n",
    "\n",
    "vs_att_decoder = vs_att_decoder.to(params.device)\n",
    "vs_att_decoder.eval()\n",
    "\n",
    "# Load word map (word2ix)\n",
    "word_map_file = os.path.join(params.data_folder, 'WORDMAP_' + params.data_name + '.json')\n",
    "with open(word_map_file, 'r') as j:\n",
    "    word_map = json.load(j)\n",
    "rev_word_map = {v: k for k, v in word_map.items()}\n",
    "vocab_size = len(word_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['http://images.cocodataset.org/val2014/COCO_val2014_000000341010.jpg']\n",
      "[['a man wearing a bright orange robe holding an umbrella', 'a monk holding an umbrella looking at a cellphone on a street', 'a monk walking down a street holding an umbrella', 'a man wearing an orange wrap holds an umbrella over himself as he uses a cell phone', 'a man in orange garb carrying a umbrella and cell phone']]\n",
      "['a man holding an umbrella on a street']\n",
      "{'Bleu_1': 0.8824969023639716, 'Bleu_2': 0.7458460118109875, 'Bleu_3': 0.5469698982480053, 'Bleu_4': 7.330918071303027e-05, 'METEOR': 0.28306350342063735, 'ROUGE_L': 0.6756329113924051, 'CIDEr': 0.0}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Evaluation\n",
    ":param beam_size: beam size at which to generate captions for evaluation\n",
    ":return: Official MSCOCO evaluator scores - bleu4, cider, rouge, meteor\n",
    "\"\"\"\n",
    "# DataLoader\n",
    "loader = torch.utils.data.DataLoader(\n",
    "    CaptionDataset(params.data_folder, params.data_name, 'TEST'),\n",
    "    batch_size=1, \n",
    "    shuffle=True,\n",
    "    num_workers=1, \n",
    "    pin_memory=torch.cuda.is_available())\n",
    "\n",
    "# Lists to store references (true captions), and hypothesis (prediction) for each image\n",
    "# If for n images, we have n hypotheses, and references a, b, c... for each image, we need -\n",
    "# references = [[ref1a, ref1b, ref1c], [ref2a, ref2b], ...], hypotheses = [hyp1, hyp2, ...]\n",
    "references = list()\n",
    "hypotheses = list()\n",
    "\n",
    "# For each image\n",
    "for i, (_, image_features, caps, caplens, attributes, allcaps) in enumerate(tqdm(loader, desc=\"EVALUATING AT BEAM SIZE \" + str(params.beam_size))):\n",
    "    # image_features, caps, caplens, attributes, allcaps = next(iter(loader))\n",
    "    image_features = image_features.to(params.device)\n",
    "    caps = caps.to(params.device)\n",
    "    caplens = caplens.to(params.device)\n",
    "    attributes = attributes.to(params.device)\n",
    "\n",
    "    k = params.beam_size\n",
    "\n",
    "    # Move to GPU device, if available\n",
    "    image_features = image_features.to(params.device)  # (1, 3, 256, 256)\n",
    "    image_features_mean = image_features.mean(1)\n",
    "    image_features_mean = image_features_mean.expand(k, params.features_dim)\n",
    "\n",
    "    # Tensor to store top k previous words at each step; now they're just <start>\n",
    "    k_prev_words = torch.LongTensor([[word_map['<start>']]] * k).to(params.device)  # (k, 1)\n",
    "\n",
    "    # Tensor to store top k sequences; now they're just <start>\n",
    "    seqs = k_prev_words  # (k, 1)\n",
    "\n",
    "    # Tensor to store top k sequences' scores; now they're just 0\n",
    "    top_k_scores = torch.zeros(k, 1).to(params.device)  # (k, 1)\n",
    "\n",
    "    # Lists to store completed sequences and scores\n",
    "    complete_seqs = list()\n",
    "    complete_seqs_scores = list()\n",
    "\n",
    "    # Start decoding\n",
    "    step = 1\n",
    "\n",
    "    h1_v, c1_v = vs_att_decoder.init_hidden_state(k)  # (batch_size, decoder_dim)\n",
    "    h1_s, c1_s = vs_att_decoder.init_hidden_state(k)  # (batch_size, decoder_dim)\n",
    "    h2, c2 = vs_att_decoder.init_hidden_state(k)  # (batch_size, decoder_dim)\n",
    "\n",
    "    attr_embedding = vs_att_decoder.attr_embedding(attributes)\n",
    "    attr_embedding_mean = attr_embedding.mean(1)\n",
    "    attr_embedding_mean = attr_embedding_mean.expand(k, params.attr_emb_dim)\n",
    "\n",
    "    # s is a number less than or equal to k, because sequences are removed from this process once they hit <end>\n",
    "    while True:\n",
    "\n",
    "        cap_embeddings = vs_att_decoder.cap_embedding(k_prev_words).squeeze(1)  # (s, embed_dim)\n",
    "        h1_v, c1_v = vs_att_decoder.top_down_visual_attention(\n",
    "            torch.cat([h2, image_features_mean, cap_embeddings], dim=1),\n",
    "            (h1_v, c1_v))  # (batch_size_t, decoder_dim)\n",
    "\n",
    "        visual_attention_weighted_encoding = vs_att_decoder.visual_attention(image_features, h1_v)\n",
    "\n",
    "        h1_s, c1_s = vs_att_decoder.top_down_sematic_attention(\n",
    "            torch.cat([h2, attr_embedding_mean, cap_embeddings], dim=1), \n",
    "            (h1_s, c1_s))\n",
    "\n",
    "        semantic_attention_weighted_encoding = vs_att_decoder.semantic_attention(attr_embedding_mean, h1_s)\n",
    "\n",
    "\n",
    "        h2, c2 = vs_att_decoder.language_model(\n",
    "            torch.cat([visual_attention_weighted_encoding, h1_v, semantic_attention_weighted_encoding, h1_s], dim=1), (h2, c2))\n",
    "\n",
    "        scores = vs_att_decoder.fc(h2)  # (s, vocab_size)\n",
    "        scores = F.log_softmax(scores, dim=1)\n",
    "\n",
    "        # Add\n",
    "        scores = top_k_scores.expand_as(scores) + scores  # (s, vocab_size)\n",
    "\n",
    "        # For the first step, all k points will have the same scores (since same k previous words, h, c)\n",
    "        if step == 1:\n",
    "            top_k_scores, top_k_words = scores[0].topk(k, 0, True, True)  # (s)\n",
    "        else:\n",
    "            # Unroll and find top scores, and their unrolled indices\n",
    "            top_k_scores, top_k_words = scores.view(-1).topk(k, 0, True, True)  # (s)\n",
    "\n",
    "        # Convert unrolled indices to actual indices of scores\n",
    "        prev_word_inds = top_k_words / vocab_size  # (s)\n",
    "        next_word_inds = top_k_words % vocab_size  # (s)\n",
    "\n",
    "        # Add new words to sequences\n",
    "        seqs = torch.cat([seqs[prev_word_inds], next_word_inds.unsqueeze(1)], dim=1)  # (s, step+1)\n",
    "\n",
    "        # Which sequences are incomplete (didn't reach <end>)?\n",
    "        incomplete_inds = [ind for ind, next_word in enumerate(next_word_inds) if\n",
    "                           next_word != word_map['<end>']]\n",
    "        complete_inds = list(set(range(len(next_word_inds))) - set(incomplete_inds))\n",
    "\n",
    "        # Set aside complete sequences\n",
    "        if len(complete_inds) > 0:\n",
    "            complete_seqs.extend(seqs[complete_inds].tolist())\n",
    "            complete_seqs_scores.extend(top_k_scores[complete_inds])\n",
    "        k -= len(complete_inds)  # reduce beam length accordingly\n",
    "\n",
    "        # Proceed with incomplete sequences\n",
    "        if k == 0:\n",
    "            break\n",
    "\n",
    "        seqs = seqs[incomplete_inds]\n",
    "        h1_v = h1_v[prev_word_inds[incomplete_inds]]\n",
    "        c1_v = c1_v[prev_word_inds[incomplete_inds]]\n",
    "        h1_s = h1_s[prev_word_inds[incomplete_inds]]\n",
    "        c1_s = c1_s[prev_word_inds[incomplete_inds]]\n",
    "        h2 = h2[prev_word_inds[incomplete_inds]]\n",
    "        c2 = c2[prev_word_inds[incomplete_inds]]\n",
    "        image_features_mean = image_features_mean[prev_word_inds[incomplete_inds]]\n",
    "        attr_embedding_mean = attr_embedding_mean[prev_word_inds[incomplete_inds]]\n",
    "        top_k_scores = top_k_scores[incomplete_inds].unsqueeze(1)\n",
    "        k_prev_words = next_word_inds[incomplete_inds].unsqueeze(1)\n",
    "\n",
    "        # Break if things have been going on too long\n",
    "        if step > 50:\n",
    "            break\n",
    "        step += 1\n",
    "\n",
    "    i = complete_seqs_scores.index(max(complete_seqs_scores))\n",
    "    seq = complete_seqs[i]\n",
    "\n",
    "    # References\n",
    "    img_caps = allcaps[0].tolist()\n",
    "    img_captions = list(\n",
    "        map(lambda c: [rev_word_map[w] for w in c if w not in {word_map['<start>'], word_map['<end>'], word_map['<pad>']}],\n",
    "            img_caps))  # remove <start> and pads\n",
    "    img_caps = [' '.join(c) for c in img_captions]\n",
    "    references.append(img_caps)\n",
    "\n",
    "    # Hypotheses\n",
    "    hypothesis = ([rev_word_map[w] for w in seq if w not in {word_map['<start>'], word_map['<end>'], word_map['<pad>']}])\n",
    "    hypothesis = ' '.join(hypothesis)\n",
    "    hypotheses.append(hypothesis)\n",
    "    assert len(references) == len(hypotheses)\n",
    "    \n",
    "# Calculate scores\n",
    "metrics_dict = nlgeval.compute_metrics(references, hypotheses)\n",
    "print(metrics_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
